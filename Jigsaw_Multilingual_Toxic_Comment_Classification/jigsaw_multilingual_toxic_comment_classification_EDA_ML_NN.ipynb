{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem recap:\n",
    "\n",
    "Ref: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/overview\n",
    "        \n",
    "It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet.\n",
    "\n",
    "In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data.\n",
    "\n",
    "Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.\n",
    "\n",
    "As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> In this Notebook, we will perform EDA of the data, check the potentials of simple classical ML models, and check with some of the SOTA NN models.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files  9\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "../../data/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../../data/jigsaw-multilingual-toxic-comment-classification\"\n",
    "for dirname, _, filenames in os.walk(base_path):\n",
    "    print(\"total files \",len(filenames))\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow.keras as keras\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import model_from_json \n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "\n",
    "max_len = 1200\n",
    "batch_size = 400*strategy.num_replicas_in_sync\n",
    "\n",
    "epoch = 5\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(base_path, 'jigsaw-toxic-comment-train.csv'))\n",
    "validation = pd.read_csv(os.path.join(base_path, 'validation.csv'))\n",
    "test = pd.read_csv(os.path.join(base_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Este usuario ni siquiera llega al rango de    ...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Il testo di questa voce pare esser scopiazzato...</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Vale. Sólo expongo mi pasado. Todo tiempo pasa...</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bu maddenin alt başlığı olarak  uluslararası i...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Belçika nın şehirlerinin yanında ilçe ve belde...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text lang  toxic\n",
       "0   0  Este usuario ni siquiera llega al rango de    ...   es      0\n",
       "1   1  Il testo di questa voce pare esser scopiazzato...   it      0\n",
       "2   2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1\n",
       "3   3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0\n",
       "4   4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            content lang\n",
       "0   0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1   1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2   2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n",
    "validation.drop(['id', 'lang'], axis=1, inplace=True)\n",
    "X = train.append(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321\n"
     ]
    }
   ],
   "source": [
    "# size_data = 50000\n",
    "# X = X.loc[:size_data, :]\n",
    "\n",
    "# max length of string\n",
    "print(train.comment_text.apply(lambda x: len(str(x).split())).max())\n",
    "def roc_auc(predictions,target):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X.comment_text.values, X.toxic.values, \n",
    "                                                stratify=X.toxic.values, \n",
    "                                                random_state=42, \n",
    "                                                test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "token = text.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dur-rbaral-m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/dur-rbaral-m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/dur-rbaral-m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1200, 300)         107498400 \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 107,538,601\n",
      "Trainable params: 107,538,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 185239 samples, validate on 46310 samples\n",
      "Epoch 1/5\n",
      "185239/185239 [==============================] - 2252s 12ms/sample - loss: 0.3755 - tp: 15176.0000 - fp: 26548.0000 - tn: 140600.0000 - fn: 2915.0000 - accuracy: 0.8409 - precision: 0.3637 - recall: 0.8389 - auc: 0.9141 - val_loss: 0.2793 - val_tp: 3928.0000 - val_fp: 4164.0000 - val_tn: 37623.0000 - val_fn: 595.0000 - val_accuracy: 0.8972 - val_precision: 0.4854 - val_recall: 0.8685 - val_auc: 0.9416\n",
      "Epoch 2/5\n",
      "185239/185239 [==============================] - 2128s 11ms/sample - loss: 0.1780 - tp: 16905.0000 - fp: 11056.0000 - tn: 156092.0000 - fn: 1186.0000 - accuracy: 0.9339 - precision: 0.6046 - recall: 0.9344 - auc: 0.9801 - val_loss: 0.3059 - val_tp: 3849.0000 - val_fp: 4481.0000 - val_tn: 37306.0000 - val_fn: 674.0000 - val_accuracy: 0.8887 - val_precision: 0.4621 - val_recall: 0.8510 - val_auc: 0.9303\n",
      "Epoch 3/5\n",
      "185239/185239 [==============================] - 2102s 11ms/sample - loss: 0.1385 - tp: 17220.0000 - fp: 8367.0000 - tn: 158781.0000 - fn: 871.0000 - accuracy: 0.9501 - precision: 0.6730 - recall: 0.9519 - auc: 0.9874 - val_loss: 0.2519 - val_tp: 3667.0000 - val_fp: 3377.0000 - val_tn: 38410.0000 - val_fn: 856.0000 - val_accuracy: 0.9086 - val_precision: 0.5206 - val_recall: 0.8107 - val_auc: 0.9336\n",
      "Epoch 4/5\n",
      "185239/185239 [==============================] - 2041s 11ms/sample - loss: 0.0887 - tp: 17641.0000 - fp: 5646.0000 - tn: 161502.0000 - fn: 450.0000 - accuracy: 0.9671 - precision: 0.7575 - recall: 0.9751 - auc: 0.9940 - val_loss: 0.2589 - val_tp: 3625.0000 - val_fp: 2783.0000 - val_tn: 39004.0000 - val_fn: 898.0000 - val_accuracy: 0.9205 - val_precision: 0.5657 - val_recall: 0.8015 - val_auc: 0.9342\n",
      "Epoch 5/5\n",
      "185239/185239 [==============================] - 2061s 11ms/sample - loss: 0.0591 - tp: 17841.0000 - fp: 3885.0000 - tn: 163263.0000 - fn: 250.0000 - accuracy: 0.9777 - precision: 0.8212 - recall: 0.9862 - auc: 0.9966 - val_loss: 0.2501 - val_tp: 3452.0000 - val_fp: 2204.0000 - val_tn: 39583.0000 - val_fn: 1071.0000 - val_accuracy: 0.9293 - val_precision: 0.6103 - val_recall: 0.7632 - val_auc: 0.9263\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                    300,\n",
    "                    input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'),\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "def save_model(model):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def load_model():\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "weightList = list(train.toxic.value_counts())\n",
    "toxic_weights = (1/weightList[1]) * ((weightList[1] + weightList[0])/2)\n",
    "nontoxic_weights = (1/weightList[0]) * ((weightList[1] + weightList[0])/2)\n",
    "class_weights = {0:nontoxic_weights, 1:toxic_weights}\n",
    "\n",
    "if os.path.exists(\"model.h5\") and os.path.exists('model.json'):\n",
    "    model = load_model()\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(xtrain_pad, ytrain, epochs=epoch, batch_size=batch_size, class_weight=class_weights, validation_data=(xvalid_pad, yvalid))\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc: 0.93%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = token.texts_to_sequences(test.content)\n",
    "test_data_seq = sequence.pad_sequences(test_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63812/63812 [==============================] - 729s 11ms/sample\n"
     ]
    }
   ],
   "source": [
    "test['toxic'] = model.predict(test_data_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'toxic']].to_csv(os.path.join(base_path, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models/notebook\n",
    "\n",
    "https://www.kaggle.com/joydeb28/simplernn-achive-good-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
